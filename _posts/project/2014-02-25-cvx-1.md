---
layout: post
title: 凸优化眼里的世界（一）
description: 自从学了最优化，世界都变简单了。
category: project
---

自从学了最优化，世界都变简单了。这次我们来介绍最优化方法在一些统计问题中的身影。不过，这么说，似乎把最优化看作是统计的工具了？哈哈，我还是喜欢优化多一些，不过...不管啦。我们开始。

##最大似然估计 Maximum Likelihood Estimation

最大似然估计最早由德国数学家高斯在1821年针对正态分布提出的，但一般将之归功于费希尔，因为费希尔在1922年再次提出了这种想法并证明了它的一些性质而使得最大似然法得到了广泛应用。
我们先来一个例子：

设产品分为合格品、不合格品，我们用随机变量X来表示某产品检查后的不合格品数目；X=0表示合格品，X=1表示不合格品，则X服从两点分布 b(1,p) 其中参数p是未知的不合格品率。
现在，抽取 n 个产品，便得到样本 x_1, x_2, ... ,x_n

这一组观测值发生的概率：
![1](/images/cvx-1/1.png)
嗯，现在的问题是，我们想知道产品的不合格率p，那么便可以用最大似然的思想去处理这个问题。为什么？

再看一个例子：

假设一种很极端的情况，有两个箱子中各有100个球。其中1号箱有99个红球、1个白球；2号箱有1个红球、99个白球。抽取一次，得到红球。
那么，你觉得这个红球是来自哪个箱子呢？我们大概都会说是1号箱子，因为那里的红球多嘛，可能性大。

那么同样地，我们有理由认为问题中的数据(x1,x2,...,xn)确实反映了不合格品的情况，并且！这个反映是充分的。**即p的取值会让这个概率最大。**

那么，目标函数就有了。最优化她要出场了...

##1.Problem Formulation.
在CVX的世界里，你只需要把问题的本质抓住，把问题formulate出来；剩下的工作就好办了。当然，有时候第一次写出的式子可能是非凸的（non-convex），或是拟凸的（quasi-convex）对于其中的情形，有方法进行转化或求解，我们以后会说到。

一般地，最大似然估计在优化的眼里就是这样一个式子：
![2](/images/cvx-1/2.png)

如果似然函数l(x)是concave的话，此优化问题便是convex optimization problem

##2.应用
好戏终于要来了。我们将穿行于最优化和统计之间，分别从相对的角度去看某个领域

###2.1Linear Measurement with IID noise
考虑一个带有噪声的线性估计：
![3](/images/cvx-1/3.png)

其中v_i是独立同分布的数据噪声，那么y_i便也是一个随机变量，不过它的具体分布是由参数x给出的，下面考虑常见噪声的分布以及优化目标解释出来的东西：

* Gaussian noise.
![4](/images/cvx-1/4.png)

假设此时v_i服从均值0、方差sigma^2的正态分布，那么密度函数以及对应的log-likelihood function是
![5](/images/cvx-1/5.png)

![6](/images/cvx-1/6.png)


你会看到，虽然我们是从统计问题出发，得到一个优化形式的问题；更重要的是，第二项是一个最小二乘（least-squares approximation）的表达（事实上第一项无关紧要）。这意味着，（如果我们反过来看）当我们用least-squares去做近似时，这个形式的暗含着我们的近似误差是服从正态分布的！ 机器学习中的linear regression问题正是这个形式。

* Laplacian noise.
![7](/images/cvx-1/7.png)

假设v_i服从Laplacian noise，密度函数和似然函数是：
![8](/images/cvx-1/8.png)

![9](/images/cvx-1/9.png)

同样地，此时我们做的就是l_1-norm approximation. 再看看分布与Gaussian的对比：
![10](/images/cvx-1/10.png)

**much heavier tail.** 后者的尾部显然比正态分布大多了，也就是说，**我们下次描述具有肥尾情况的问题时，或许采取l1-norm approximation会有更好的效果？**

**这里提一个疑问**：如果我对股指期货的数据作为研究对象，我用l1-norm approximation的方式进行linear regression好些，还是least-squares呢？因为我选择l1-norm的原因是它更接近股市中的长尾情况。不过，这样的大误差也会不时出现，这是对于策略设计需要如何考虑？

下面这幅图可以帮助大家理解不同的penalties对residuals的影响：
![11](/images/cvx-1/11.png)
*图片来源. Stephen Boyd. Convex Optimization. CVX101 - Stanford Online.

可以看到l1-norm的0误差数目很多，不过大误差的也总是有出现。而least-squares误差都比较集中，不过总的近似情况就稍逊一筹。


[zihaolucky]:    http://zihaolucky.github.io  "zihaolucky"
[1]:    {{ page.url}}  ({{ page.title }})
